{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark  \n",
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('HelloWorld').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/d:/Educational Others/2023 Data Engineering Zoomcamp/resources/datasets/fhvhv_tripdata_2022-01.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Educational Others\\2023 Data Engineering Zoomcamp\\spark\\04_pyspark.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp/spark/04_pyspark.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Read parquet spark\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp/spark/04_pyspark.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mparquet(\u001b[39m\"\u001b[39;49m\u001b[39m../resources/datasets/fhvhv_tripdata_2022-01.parquet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mC:\\Program Files\\spark-3.2.3-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py:301\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[1;34m(self, *paths, **options)\u001b[0m\n\u001b[0;32m    295\u001b[0m int96RebaseMode \u001b[39m=\u001b[39m options\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mint96RebaseMode\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    296\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(mergeSchema\u001b[39m=\u001b[39mmergeSchema, pathGlobFilter\u001b[39m=\u001b[39mpathGlobFilter,\n\u001b[0;32m    297\u001b[0m                recursiveFileLookup\u001b[39m=\u001b[39mrecursiveFileLookup, modifiedBefore\u001b[39m=\u001b[39mmodifiedBefore,\n\u001b[0;32m    298\u001b[0m                modifiedAfter\u001b[39m=\u001b[39mmodifiedAfter, datetimeRebaseMode\u001b[39m=\u001b[39mdatetimeRebaseMode,\n\u001b[0;32m    299\u001b[0m                int96RebaseMode\u001b[39m=\u001b[39mint96RebaseMode)\n\u001b[1;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mparquet(_to_seq(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc, paths)))\n",
      "File \u001b[1;32mC:\\Program Files\\spark-3.2.3-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Program Files\\spark-3.2.3-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Path does not exist: file:/d:/Educational Others/2023 Data Engineering Zoomcamp/resources/datasets/fhvhv_tripdata_2022-01.parquet"
     ]
    }
   ],
   "source": [
    "# Read parquet spark\n",
    "\n",
    "df = spark.read.parquet(\"../resources/datasets/fhvhv_tripdata_2022-01.parquet\")\n",
    "\n",
    "#You can also create dataframe from Pandas\n",
    "# spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Print S\n",
    "# df.display()\n",
    "# df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count:  14751591\n"
     ]
    }
   ],
   "source": [
    "#You can also specify schema\n",
    "df.schema # schema of the dataframe\n",
    "\n",
    "from pyspark.sql import types\n",
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True), \n",
    "    types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField('originating_base_num', types.StringType(), True), \n",
    "    types.StructField('request_datetime', types.TimestampType(), True), \n",
    "    types.StructField('on_scene_datetime', types.TimestampType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('PULocationID', types.LongType(), True), \n",
    "    types.StructField('DOLocationID', types.LongType(), True), \n",
    "    types.StructField('trip_miles', types.DoubleType(), True), \n",
    "    types.StructField('trip_time', types.LongType(), True), \n",
    "    types.StructField('base_passenger_fare', types.DoubleType(), True), \n",
    "    types.StructField('tolls', types.DoubleType(), True), \n",
    "    types.StructField('bcf', types.DoubleType(), True), \n",
    "    types.StructField('sales_tax', types.DoubleType(), True), \n",
    "    types.StructField('congestion_surcharge', types.DoubleType(), True), \n",
    "    types.StructField('airport_fee', types.DoubleType(), True), \n",
    "    types.StructField('tips', types.DoubleType(), True), \n",
    "    types.StructField('driver_pay', types.DoubleType(), True), \n",
    "    types.StructField('shared_request_flag', types.StringType(), True), \n",
    "    types.StructField('shared_match_flag', types.StringType(), True), \n",
    "    types.StructField('access_a_ride_flag', types.StringType(), True), \n",
    "    types.StructField('wav_request_flag', types.StringType(), True), \n",
    "    types.StructField('wav_match_flag', types.StringType(), True)\n",
    "])\n",
    "\n",
    "#You can only define schema during the load\n",
    "df = spark.read.schema(schema).parquet(\"../resources/datasets/fhvhv_tripdata_2022-01.parquet\")\n",
    "print(\"Row count: \", df.count())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repartitions\n",
    "When reading a single large file, Spark will read the file in a single partition. This can be a problem if the file is too large to fit in memory. In this case, we can repartition the data into smaller partitions. This can be done by using the `repartition` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(4)\n",
    "# df = df.coalesce(1)\n",
    "\n",
    "# Now, the file is partitioned into 24 but will be saved as 1\n",
    "# There would be a temporary directory created in the process\n",
    "df.write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"../resources/datasets/fhvhv_tripdata_2022-01.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading multiple files\n",
    "Spark can read multiple files at once. This can be done by using the `glob` method to read all files that match a pattern. For example, to read all files that end with `.csv`, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count:  14751591\n",
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: string (nullable = true)\n",
      " |-- on_scene_datetime: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- trip_miles: string (nullable = true)\n",
      " |-- trip_time: string (nullable = true)\n",
      " |-- base_passenger_fare: string (nullable = true)\n",
      " |-- tolls: string (nullable = true)\n",
      " |-- bcf: string (nullable = true)\n",
      " |-- sales_tax: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      " |-- airport_fee: string (nullable = true)\n",
      " |-- tips: string (nullable = true)\n",
      " |-- driver_pay: string (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../resources/datasets/fhvhv_tripdata_2022-01.csv/', header=True)\n",
    "\n",
    "# Get number of rows\n",
    "print(\"Row count: \", df.count())\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting columns\n",
    "Similar to SQL, we can select columns using the `select` method. This method takes a list of column names as arguments.\n",
    "\n",
    "**Transformations** are things that are not executed right away (laz)\n",
    "* `Selecting` columns\n",
    "* `Filtering` rows\n",
    "* `Grouping` rows\n",
    "* `Joins` between tables\n",
    "\n",
    "**Actions** are things that are executed right away\n",
    "* `show`\n",
    "* `take`\n",
    "* `head`\n",
    "* `write`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----+\n",
      "|hvfhs_license_num|dispatching_base_num|tips|\n",
      "+-----------------+--------------------+----+\n",
      "|           HV0003|              B03404| 0.0|\n",
      "|           HV0003|              B03404| 0.0|\n",
      "|           HV0003|              B03404| 0.0|\n",
      "|           HV0003|              B03404| 0.0|\n",
      "|           HV0003|              B03404| 0.0|\n",
      "+-----------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('hvfhs_license_num', 'dispatching_base_num','tips').filter(df.hvfhs_license_num == 'HV0003').show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use SQL queries to select columns. This can be done by registering the DataFrame as a temporary table and then using the `sql` method to execute the query.\n",
    "\n",
    "## Pyspark Functions\n",
    "Pyspark has a lot of built-in functions that can be used to manipulate data. These functions can be found in the `pyspark.sql.functions` module. The following code shows how to import the module and use some of the functions.\n",
    "Documentation: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n",
    "\n",
    "Notable functions in the video:\n",
    "`pyspark.sql.functions.udf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2022-01-10|  2022-01-10|         188|         231|\n",
      "| 2022-01-22|  2022-01-22|          87|         137|\n",
      "| 2022-01-20|  2022-01-20|         231|          97|\n",
      "| 2022-01-25|  2022-01-25|         116|         159|\n",
      "| 2022-01-04|  2022-01-04|          68|         148|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df.withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime))\\\n",
    "    .select('pickup_date', 'dropoff_date','PULocationID','DOLocationID')\\\n",
    "    .show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
