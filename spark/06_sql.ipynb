{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark  \n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName('HelloWorld')\\\n",
    "    .config(\"spark.driver.memory\", \"6G\") \\\n",
    "    .config(\"spark.executor.memory\", \"6G\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"6G\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DOLocationID',\n",
       " 'PULocationID',\n",
       " 'RatecodeID',\n",
       " 'VendorID',\n",
       " 'congestion_surcharge',\n",
       " 'extra',\n",
       " 'fare_amount',\n",
       " 'improvement_surcharge',\n",
       " 'mta_tax',\n",
       " 'passenger_count',\n",
       " 'payment_type',\n",
       " 'store_and_fwd_flag',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'total_amount',\n",
       " 'trip_distance'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yellow = spark.read.parquet(\"../resources/datasets/yellow/*/*\")\n",
    "df_green = spark.read.parquet(\"../resources/datasets/green/*/*\")\n",
    "\n",
    "set(df_yellow.columns) & set(df_green.columns) # intersection of columns using & operator\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows `lpep_pickup_datetime` and `lpep_dropoff_datetime` are not present in the intersection.  We can rename the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common columns:  {'PULocationID', 'passenger_count', 'improvement_surcharge', 'congestion_surcharge', 'payment_type', 'dropoff_datetime', 'tip_amount', 'mta_tax', 'extra', 'VendorID', 'RatecodeID', 'total_amount', 'DOLocationID', 'pickup_datetime', 'tolls_amount', 'trip_distance', 'fare_amount', 'store_and_fwd_flag'}\n",
      "Not in both:  {'trip_type', 'airport_fee', 'ehail_fee'}\n"
     ]
    }
   ],
   "source": [
    "df_green = df_green\\\n",
    "    .withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\") \\\n",
    "    .withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "\n",
    "#Same with yellow\n",
    "df_yellow = df_yellow\\\n",
    "    .withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\") \\\n",
    "    .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "\n",
    "common_columns = set(df_yellow.columns) & set(df_green.columns) # intersection of columns using & operator\n",
    "print(\"Common columns: \", common_columns)\n",
    "print(\"Not in both: \", set(df_green.columns).symmetric_difference(set(df_yellow.columns))) # symmetric difference of columns using symmetric_difference() method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding `service_type` column to differentiate the two types of taxi services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = df_yellow.select(*common_columns).withColumn(\"service_type\", F.lit(\"yellow\"))\n",
    "df_green = df_green.select(*common_columns).withColumn(\"service_type\", F.lit(\"green\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now union the two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow count:  84598444\n",
      "Green count:  6300985\n",
      "Union count:  90899429\n",
      "Alternative way: \n",
      "+------------+--------+\n",
      "|service_type|   count|\n",
      "+------------+--------+\n",
      "|       green| 6300985|\n",
      "|      yellow|84598444|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trips_data = df_green.unionAll(df_yellow)\n",
    "print(\"Yellow count: \", df_yellow.count())\n",
    "print(\"Green count: \", df_green.count())\n",
    "print(\"Union count: \", df_trips_data.count())\n",
    "\n",
    "print(\"Alternative way: \")\n",
    "df_trips_data.groupBy(\"service_type\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dim zones for use later in joins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_zones  = spark.read.csv(\"../resources/datasets/taxi+_zone_lookup.csv\", header=True)\n",
    "dim_zones.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary Tables\n",
    "Pyspark can create temporary tables from dataframes.  These tables are only available to the current session.\n",
    "\n",
    "Note: SQL statement is copied and modified from `fact_trips.sql` and `dim_monthly_zone_revenue.sql` from Week 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_data.createOrReplaceTempView('trips_data')\n",
    "dim_zones.createOrReplaceTempView('dim_zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equivalent to fact_trips.sql\n",
    "fact_trips_sql = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    -- trips_data.tripid,\n",
    "    trips_data.vendorid,\n",
    "    trips_data.service_type,\n",
    "    trips_data.ratecodeid,\n",
    "    trips_data.PULocationID,\n",
    "    pickup_zone.borough AS pickup_borough,\n",
    "    pickup_zone.zone AS pickup_zone,\n",
    "    trips_data.DOLocationID,\n",
    "    dropoff_zone.borough AS dropoff_borough,\n",
    "    dropoff_zone.zone AS dropoff_zone,\n",
    "\n",
    "    trips_data.pickup_datetime,\n",
    "    trips_data.dropoff_datetime,\n",
    "    trips_data.store_and_fwd_flag,\n",
    "    trips_data.passenger_count,\n",
    "    trips_data.trip_distance,\n",
    "    -- trips_data.trip_type, \n",
    "    trips_data.fare_amount, \n",
    "    trips_data.extra, \n",
    "    trips_data.mta_tax, \n",
    "    trips_data.tip_amount, \n",
    "    trips_data.tolls_amount, \n",
    "    -- trips_data.ehail_fee, \n",
    "    trips_data.improvement_surcharge, \n",
    "    trips_data.total_amount, \n",
    "    trips_data.payment_type, \n",
    "    -- trips_data.payment_type_description, \n",
    "    trips_data.congestion_surcharge\n",
    "FROM trips_data\n",
    "INNER JOIN dim_zones AS pickup_zone\n",
    "    ON trips_data.PULocationID = pickup_zone.locationid\n",
    "INNER JOIN dim_zones AS dropoff_zone\n",
    "    ON trips_data.DOLocationID = dropoff_zone.locationid\n",
    "\"\"\")\n",
    "fact_trips_sql.createOrReplaceTempView('fact_trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/d:/Educational Others/2023 Data Engineering Zoomcamp/resources/datasets/dm_monthly_zone_revenue.parquet already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Educational Others\\2023 Data Engineering Zoomcamp\\spark\\06_sql.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp/spark/06_sql.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Equivalent to dm_monthly_zone_revenue.sql\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp/spark/06_sql.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dm_monthly_zone_revenue_sql \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39msql(\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp/spark/06_sql.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mSELECT\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp/spark/06_sql.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m    pickup_zone AS revenue_zone,\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp/spark/06_sql.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mGROUP BY pickup_zone, MONTH(pickup_datetime), service_type\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp/spark/06_sql.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mORDER BY pickup_zone, MONTH(pickup_datetime), service_type\u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Educational%20Others/2023%20Data%20Engineering%20Zoomcamp/spark/06_sql.ipynb#X20sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m dm_monthly_zone_revenue_sql\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mparquet(\u001b[39m\"\u001b[39;49m\u001b[39m../resources/datasets/dm_monthly_zone_revenue.parquet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mC:\\Program Files\\spark-3.2.3-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py:885\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m    883\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m    884\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[1;32m--> 885\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[1;32mC:\\Program Files\\spark-3.2.3-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Program Files\\spark-3.2.3-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/d:/Educational Others/2023 Data Engineering Zoomcamp/resources/datasets/dm_monthly_zone_revenue.parquet already exists."
     ]
    }
   ],
   "source": [
    "#Equivalent to dm_monthly_zone_revenue.sql\n",
    "dm_monthly_zone_revenue_sql = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    pickup_zone AS revenue_zone,\n",
    "    MONTH(pickup_datetime) AS month,\n",
    "    service_type,\n",
    "\n",
    "    SUM(fare_amount) AS revenue_monthly_fare,\n",
    "    SUM(extra) AS revenue_monthly_extra,\n",
    "    SUM(mta_tax) AS revenue_monthly_mta_tax,\n",
    "    SUM(tip_amount) AS revenue_monthly_tip_amount,\n",
    "    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n",
    "    -- SUM(ehail_fee) AS revenue_monthly_ehail_fee,\n",
    "    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n",
    "    SUM(total_amount) AS revenue_monthly_total_amount,\n",
    "    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n",
    "\n",
    "    COUNT(*) AS total_monthly_trips,\n",
    "    AVG(pASsenger_count) AS avg_montly_pASsenger_count,\n",
    "    AVG(trip_distance) AS avg_montly_trip_distance\n",
    "\n",
    "FROM fact_trips\n",
    "GROUP BY pickup_zone, MONTH(pickup_datetime), service_type\n",
    "ORDER BY pickup_zone, MONTH(pickup_datetime), service_type\"\"\")\n",
    "\n",
    "dm_monthly_zone_revenue_sql.write.option(\"header\", \"true\").parquet(\"../resources/datasets/dm_monthly_zone_revenue.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+-------------------+--------------------------+------------------------+\n",
      "|        revenue_zone|month|service_type|revenue_monthly_fare|revenue_monthly_extra|revenue_monthly_mta_tax|revenue_monthly_tip_amount|revenue_monthly_tolls_amount|revenue_monthly_improvement_surcharge|revenue_monthly_total_amount|revenue_monthly_congestion_surcharge|total_monthly_trips|avg_montly_pASsenger_count|avg_montly_trip_distance|\n",
      "+--------------------+-----+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+-------------------+--------------------------+------------------------+\n",
      "|Allerton/Pelham G...|    1|       green|  24585.560000000052|                605.0|                  419.5|        18.060000000000002|          1188.0799999999983|                   139.79999999999953|          26957.949999999895|                                 0.0|                865|        1.3477611940298508|       7.795699421965319|\n",
      "|Allerton/Pelham G...|    1|      yellow|   8052.049999999994|               166.75|                  101.0|                     10.32|           527.3999999999995|                    68.39999999999972|           8927.870000000012|                                 0.0|                232|        1.4915254237288136|      10.243232758620683|\n",
      "|Allerton/Pelham G...|    2|       green|  28440.380000000045|                640.5|                  445.0|                     43.53|          1486.7999999999993|                   141.59999999999962|          31214.309999999874|                               19.25|                913|        1.2778561354019746|       8.685531215772187|\n",
      "|Allerton/Pelham G...|    2|      yellow|   7420.389999999994|               147.25|                  100.5|                     45.75|           517.7999999999995|                    67.19999999999972|            8306.39000000001|                                 7.5|                224|        1.4252873563218391|       9.290401785714286|\n",
      "|Allerton/Pelham G...|    3|       green|  23891.810000000034|                492.5|                  405.0|                     31.32|          1263.6199999999992|                   136.19999999999942|           26227.89999999988|                                8.25|                835|          1.22782874617737|       8.183700598802401|\n",
      "+--------------------+-----+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+-------------------+--------------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# monthly_sql = spark.sql(\"SELECT revenue_zone, month, service_type, revenue_monthly_fare, avg_montly_trip_distance FROM dm_monthly_zone_revenue\")\n",
    "dm_monthly_zone_revenue_sql.show(5)\n",
    "dm_monthly_zone_revenue_sql.coalesce(1).write.parquet('../resources/datasets/monthly_zone_revenue.parquet', mode='overwrite')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
